import json
import os
from dotenv import load_dotenv
import asyncio
import aioboto3
from timeit import default_timer as timer

# loading in environment variables
load_dotenv()

async def get_bedrock_client():
    """
    Asynchronously creates and returns a client for interacting with the Bedrock Runtime service.

    This function uses aioboto3, an asynchronous version of the AWS SDK for Python (Boto3), to create a client
    for the Bedrock Runtime service. It retrieves the AWS profile name and region name from environment variables.

    Returns:
        aioboto3.client: A client object for interacting with the Bedrock Runtime service.
    """

    # Create an aioboto3 session using the specified profile name
    session = aioboto3.Session(profile_name=os.getenv("profile_name"))
    # Asynchronously create a client for the Bedrock Runtime service
    async with session.client(
            service_name='bedrock-runtime',
            region_name=os.getenv("region_name"),

    ) as client:
        return client

async def model_execution(client, user_prompt, modelID):
    """
    Asynchronously executes a model with the provided user prompt.

    This function uses the provided client to invoke the model asynchronously with the provided user prompt.
    It returns the output text from the model execution.

    Args:
        client (aioboto3.client): The client for interacting with the Bedrock Runtime service.
        user_prompt (str): The question inserted by the user to pass to the model.
        modelID (str): The ID of the model to execute.

    Returns:
        str: The output text from the model execution.
    """
    # Construct the prompt object with model execution parameters, and the users question, formatted for the Claude 3 Messages API
    prompt = {
        "anthropic_version": "bedrock-2023-05-31",
        "max_tokens": 10000,
        "temperature": 0,
        "messages": [
            {
                "role": "user",
                "content": [{
                    "type": "text",
                    "text": user_prompt
                }]
            }
        ]
    }
    # Convert the prompt object to a JSON string
    prompt = json.dumps(prompt)
    # Invoke the model asynchronously with the provided prompt
    response = await client.invoke_model(
        body=prompt,
        modelId=modelID,
        accept="application/json",
        contentType="application/json"
    )
    # Read the response body and parse it as JSON
    response_body = await response['body'].read()
    response_json = json.loads(response_body)
    # Extract the output text from the response
    output_text = response_json['content'][0]['text']
    # Return the output text generated by each model
    return output_text

async def main(question, modelID):
    """
    Asynchronously executes an individual model with the provided user prompt.
    Args:
        question: The question the user input on the front end.
        modelID: The specific model being used to generate an answer.

    Returns:
        A tuple containing the result, modelID and time of invocation (latency).
    """
    # Starting a timer to time the latency to invoke the individual model
    start = timer()
    # Creating a client for the Bedrock Runtime service
    async with await get_bedrock_client() as client:
        # Executing the model asynchronously with the Asynchronous Bedrock Client, the users question, and specific model
        result = await model_execution(client, question, modelID)
    # Ending the timer to time the latency to invoke the individual model
    end = timer()
    # Calculating the latency to invoke the individual model
    time_length = round(end - start, 2)
    # Returning the result, modelID and time of invocation (latency)
    return result, modelID, time_length

async def orchestrator(question, modelID1="anthropic.claude-3-sonnet-20240229-v1:0",
                       modelID2="anthropic.claude-3-haiku-20240307-v1:0", modelID3='anthropic.claude-v2:1'):
    """
    Orchestrates the execution of multiple models asynchronously.
    Args:
        question: The question the user input on the front end.
        modelID1: The first model to execute (default: "anthropic.claude-3-sonnet-20240229-v1:0").
        modelID2: The second model to execute (default: "anthropic.claude-3-haiku-20240307-v1:0").
        modelID3: The third model to execute (default: "anthropic.claude-v2:1").

    Returns:
        A list of results, each result is a tuple containing the result, modelID and time of invocation, for each model.
    """
    # Call asyncio.gather() to execute multiple models concurrently, passing the question and model IDs
    result = await asyncio.gather(main(question, modelID1), main(question, modelID2), main(question, modelID3))
    # Return the list of results, each result is a tuple containing the result, modelID and time of invocation, for each model.
    return result
